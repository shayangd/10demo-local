# Interview Execution Guide - Recipe 1.1
## Problem Validation for 10Demo

---

## Overview

### Venture Context

**Problem Statement:**
B2B SaaS prospects wait 24-120 hours (average 42 hours) for generic product demos that don't address their specific needs, while sales teams face 40-60% no-show rates and massive pipeline leakage due to scheduling friction and delays. This affects both B2B SaaS companies (sales teams overwhelmed with demo requests) and their prospects (frustrated by delays and generic presentations).

**Solution Concept:**
10Demo provides an AI agent that joins video calls to deliver personalized, voice-led product demos 24/7, responding in under 5 minutes. The AI controls and navigates the real product in real-time, adapts dynamically to prospect questions through two-way conversations, and leverages deep CRM integration for context-aware demos.

**Target Customers:**
- B2B SaaS companies with high demo request volumes
- Sales teams struggling with scheduling friction and no-shows
- Revenue teams looking to scale without adding headcount

### Interview Goals

**Primary Objectives:**
1. Validate problem severity and frequency (KPI I1.1-I1.3)
2. Assess current workarounds and competitive landscape (KPI I1.4-I1.5)
3. Gauge solution resonance and competitive displacement (KPI I1.6-I1.7)
4. Understand willingness to pay and budget dynamics (KPI I1.8-I1.10)

**Success Criteria:**
- Complete 10-15 interviews across target personas
- 70%+ report problem severity ≥7/10
- 60%+ show strong purchase intent (≥4/5)
- Clear pricing range emerges from Van Westendorp analysis
- Identify 2-3 early adopter candidates

---

## Target Personas

### Persona 1: VP/Director of Sales at Growth-Stage B2B SaaS
**Segment:** Mid-Market
**Role:** VP Sales, Director of Sales, Head of Sales
**Company Profile:**
- 50-500 employees
- $10M-$100M ARR
- Product-led or sales-led B2B SaaS
- 5-20 person sales team
- High demo volume (50-200+ demos/month)

**Why Interview:**
- Primary economic buyer with budget authority
- Directly feels pain of scheduling friction and no-shows
- Accountable for pipeline conversion and sales efficiency metrics
- Can articulate current costs (AE salaries, tools, opportunity cost)

**Interview Focus:**
- Problem severity: demo scheduling delays, no-show rates, pipeline leakage
- Current spend: demo scheduling tools, sales automation, SDR/AE costs
- Solution fit: scaling demos without headcount, 24/7 availability
- WTP: budget for sales productivity tools, ROI from faster response times

**Recommended Count:** 5 interviews (35% of total)

---

### Persona 2: Sales Operations Leader at Enterprise B2B SaaS
**Segment:** Enterprise
**Role:** VP Sales Ops, Director of Sales Ops, RevOps Leader
**Company Profile:**
- 500+ employees
- $100M+ ARR
- Complex sales cycles (3-12 months)
- 50+ person sales team
- Sophisticated tech stack with CRM, sales engagement, demo tools

**Why Interview:**
- Influences purchasing decisions for sales tech
- Deeply familiar with demo metrics and conversion data
- Owns sales process optimization and tech stack integration
- Can validate quantifiable pain points with actual metrics

**Interview Focus:**
- Quantified pain: average demo wait time, no-show rates, conversion impact
- Current stack: existing demo tools, pain points, integration requirements
- Solution evaluation: technical requirements, CRM integration, security/compliance
- Budget process: how sales tech purchases are approved, typical price ranges

**Recommended Count:** 3 interviews (20% of total)

---

### Persona 3: Founder/CEO at Early-Stage B2B SaaS
**Segment:** SMB
**Role:** Founder, CEO, Head of Growth
**Company Profile:**
- 10-50 employees
- <$10M ARR or pre-revenue
- Founder-led sales or small sales team (1-5 people)
- High growth urgency, resource constraints

**Why Interview:**
- Decision-maker with full budget authority
- Personally conducts demos or closely involved in sales
- Acutely feels resource constraints and scaling challenges
- Early adopter profile (willing to try new solutions)

**Interview Focus:**
- Founder time allocation: hours spent on demos, scheduling friction
- Scaling anxiety: concerns about hiring, training, maintaining quality
- Speed to demo: impact of response time on conversion
- Budget constraints: willingness to pay for founder time savings

**Recommended Count:** 4 interviews (25% of total)

---

### Persona 4: Sales Development Rep (SDR) Manager
**Segment:** Mid-Market
**Role:** SDR Manager, Manager of Business Development
**Company Profile:**
- 50-500 employees
- 5-15 person SDR team
- High outbound volume
- Responsible for demo booking and show-up rates

**Why Interview:**
- Directly experiences demo scheduling friction daily
- Key influencer in sales tool adoption
- Can articulate SDR productivity challenges and metrics
- Champions solutions that make their team more efficient

**Interview Focus:**
- Daily pain points: time spent scheduling, handling no-shows, follow-up
- Impact on team morale: SDR frustration with wasted effort
- Qualification process: how demos are qualified and routed
- Tool adoption: what tools SDRs currently use, what they love/hate

**Recommended Count:** 3 interviews (20% of total)

**Total Target: 15 interviews across 4 personas**

---

## Recruitment Strategy

### Recruitment Channels

**1. LinkedIn (Primary Channel - 60% of interviews)**
- **Search Strategy:**
  - Use Sales Navigator to filter by job title, company size, industry
  - Target companies with job postings for SDRs/AEs (signal of growth)
  - Look for people posting about sales challenges, demo frustrations
- **Engagement Approach:**
  - Comment on relevant posts before reaching out
  - Reference their content in outreach message
  - Offer value exchange (share learnings, early access)

**2. Existing Network (20% of interviews)**
- Personal connections in B2B SaaS sales
- Alumni network from previous companies
- Warm introductions from advisors/investors
- LinkedIn 2nd-degree connections with mutual connections

**3. SaaS Communities (15% of interviews)**
- SaaStr community members
- Pavilion (Revenue Collective) members
- Sales Hacker community
- LinkedIn groups: "SaaS Sales Leadership," "Sales Operations"

**4. Conferences & Events (5% of interviews)**
- SaaStr Annual/Regional events
- Sales 3.0 Conference
- Local SaaS/sales meetups
- Virtual webinars on sales productivity

### Outreach Templates

#### Template 1: LinkedIn Direct Message (Cold)
**Subject:** Quick question about demo scheduling

Hi [First Name],

I saw your post about [specific challenge they mentioned] and it resonated with me. I'm doing research on how B2B SaaS sales teams handle demo scheduling and would love to learn from your experience.

Would you be open to a 20-minute conversation about how you currently manage demo requests and scheduling? No pitch—I'm genuinely trying to understand the problem space and would be happy to share what I'm learning from other sales leaders.

I can work around your schedule—here's my calendar: [Calendly link]

Thanks for considering,
[Your name]

P.S. Happy to send a $50 Amazon gift card as a thank you for your time.

---

#### Template 2: Warm Introduction Email
**Subject:** Introduction from [Mutual Contact]

Hi [First Name],

[Mutual Contact] suggested I reach out to you. I'm researching how B2B SaaS sales teams handle the demo scheduling process—specifically the challenges around response time, no-shows, and scaling demo capacity.

Given your experience at [Company], I'd love to hear your perspective on:
- How you currently handle demo requests
- What's working (and what's not) in your demo process
- How scheduling friction impacts your pipeline

This is purely research—no sales pitch. The conversation would take about 20-30 minutes, and I'm happy to share insights from other sales leaders I'm talking to.

Would you be open to a quick call? Here are a few times that work for me: [specific times]

Thanks,
[Your name]

---

#### Template 3: Value-Forward LinkedIn Message
**Subject:** [Company] demo metrics benchmark

Hi [First Name],

I'm conducting research on B2B SaaS demo processes and have spoken with [X] sales leaders at companies like [Company A], [Company B], and [Company C].

I'm seeing some interesting patterns around demo response times (average 42 hours), no-show rates (40-60%), and impact on pipeline velocity. I thought you might find the benchmark data useful given your role at [Company].

Would you be open to a 20-minute conversation where I can share what I'm learning in exchange for hearing about your experience? I'm specifically interested in how scaling companies handle demo volume without exploding headcount costs.

Let me know if you're interested—I can work around your schedule.

Best,
[Your name]

---

### Incentives & Value Exchange

**Incentive Options:**
1. **Benchmark Report:** Share aggregate findings from all interviews (anonymized)
2. **Gift Card:** $50 Amazon/Starbucks gift card for 30-minute interview
3. **Early Access:** Priority access to beta product (if interview leads to strong fit)
4. **Network Connection:** Intro to other sales leaders in your network
5. **Free Consultation:** 30-minute follow-up to discuss their specific demo challenges

**Recommended Approach:**
- Lead with value exchange (benchmark data) for senior buyers (VP Sales, RevOps)
- Offer gift cards for mid-level managers (SDR Managers, Sales Managers)
- Emphasize early access for founder personas (natural early adopters)

---

### Recruitment Timeline

**Week 1: Outreach Wave 1**
- Day 1-2: Identify 50 target prospects across personas
- Day 3-4: Send 30 outreach messages (LinkedIn + email)
- Day 5-7: Follow up on non-responses, schedule first 3-5 interviews

**Week 2: Outreach Wave 2 + First Interviews**
- Day 8-9: Send 20 additional outreach messages
- Day 10-14: Conduct 5-7 interviews, iterate messaging based on response rate

**Week 3: Final Outreach + Interview Sprint**
- Day 15-17: Send final 10 outreach messages to fill gaps
- Day 18-21: Conduct 5-8 interviews, focus on under-represented personas

**Week 4: Final Interviews + Early Analysis**
- Day 22-26: Conduct final 2-3 interviews
- Day 27-28: Quick review of themes to inform any follow-up questions

**Target Completion: 15 interviews in 3-4 weeks**

---

## Interview Script (27 Questions)

### Pre-Interview Setup (5 minutes before call)

**Preparation Checklist:**
- [ ] Review interviewee's LinkedIn profile and company
- [ ] Check their company's website (product, customers, pricing if public)
- [ ] Prepare 2-3 specific questions based on their background
- [ ] Test recording setup (with permission)
- [ ] Have tracking spreadsheet open
- [ ] Silence phone and close distracting tabs

**Opening Script:**
"Hi [Name], thanks so much for making time today. As I mentioned, I'm researching how B2B SaaS companies handle product demos—specifically the scheduling and delivery process. This is pure research, not a sales pitch. I'm looking to understand your real experiences and challenges.

I have about 27 questions that will take us 45-50 minutes. We'll start with your current demo process, then explore challenges and solutions. Feel free to be completely candid—there are no wrong answers.

Is it okay if I record this for my notes? The recording stays private and is only for my analysis.

Before we dive in, tell me a bit about your role and how you're involved in the demo process."

---

### SECTION 1: Problem Discovery (Q1-Q8, 20 minutes)

**Focus:** Past behavior, not hypothetical future. Probe for specifics and recency. Listen for pain intensity and workarounds.

---

#### Q1: Tell me about the last time a prospect requested a demo of your product. Walk me through exactly what happened from the moment they requested it until the demo occurred.

**Purpose:** Establishes baseline understanding of current process (feeds KPI I1.1 - Articulation Clarity)

**Probing Questions:**
- How did they request the demo? (form, email, chat, call)
- Who handled the request first?
- How long did it take to respond to them?
- How many back-and-forth messages were needed to schedule?
- What happened between scheduling and the actual demo?

**Red Flags to Listen For:**
- Vague or hypothetical responses ("Usually we..." vs specific recent example)
- No mention of friction or delays (suggests not a painful problem)
- Immediate "perfect process" description (be skeptical)

**What Good Looks Like:**
- Specific date/example ("Last week, on Tuesday...")
- Detailed step-by-step with timing
- Unprompted mention of frustrations or inefficiencies

---

#### Q2: When was the last time a demo got canceled or the prospect didn't show up? What happened?

**Purpose:** Validates frequency of no-show problem (feeds KPI I1.2 - Problem Frequency)

**Probing Questions:**
- How often does this happen? (per week/month)
- What reasons do they give? (if any)
- How much time had you invested before the no-show?
- What did you do afterward? (follow-up attempts)
- How did it impact that deal?

**Red Flags to Listen For:**
- "Rarely happens" or "Not really a problem for us"
- Can't recall a recent example
- Dismisses it as normal and not costly

**What Good Looks Like:**
- Happens frequently (multiple times per week/month)
- Clear frustration in their voice
- Can quantify cost (time wasted, deals lost)

---

#### Q3: Walk me through a time when a qualified lead went cold or chose a competitor because of delays in getting them a demo.

**Purpose:** Validates opportunity cost and revenue impact (feeds KPI I1.3 - Problem Severity)

**Probing Questions:**
- How did you know they were qualified?
- How long was the delay?
- What feedback did they give (if any)?
- Did they explicitly mention the delay as a factor?
- How much ARR did you lose?

**Red Flags to Listen For:**
- No specific examples (hypothetical)
- Attributes loss to price/features, not process
- Can't connect delay to deal loss

**What Good Looks Like:**
- Specific deal they lost
- Clear connection between delay and loss
- Shows genuine regret/frustration
- Can quantify revenue impact

---

#### Q4: On a scale of 0-10, how painful is the demo scheduling process for your team? Why that number?

**Purpose:** Quantifies pain severity (feeds KPI I1.3 - Problem Severity Score)

**Probing Questions:**
- What makes it that painful? (specific examples)
- What would make it a 10? (understand ceiling)
- What would make it a 0? (understand ideal state)
- Has it gotten better or worse in the last year?

**Red Flags to Listen For:**
- Score below 5 (not painful enough)
- Can't articulate why they chose that number
- "It's fine, we've adapted"

**What Good Looks Like:**
- Score 7+ with immediate, specific justification
- Unprompted mentions of costs (time, deals, team morale)
- Describes it as top 3 sales process pain point

---

#### Q5: Tell me about the tools and processes you've tried to solve this scheduling and demo delivery problem. What worked? What didn't?

**Purpose:** Uncovers workarounds and competitive landscape (feeds KPI I1.4 - Current Workarounds)

**Probing Questions:**
- When did you implement each solution?
- What specifically did it improve?
- What pain points remain?
- Why haven't you switched to something else?
- How much are you spending on these tools?

**Red Flags to Listen For:**
- Currently using sophisticated solution that fully solves the problem
- No workarounds (suggests low pain)
- Happy with current state

**What Good Looks Like:**
- Multiple attempted solutions
- Clear gaps in current tools
- Frustration with limitations
- Open to better alternatives

---

#### Q6: Think about the last time you had to scale up your demo capacity—maybe you were launching a new product, entering a new market, or saw a spike in inbound demand. How did you handle it?

**Purpose:** Understands scaling challenges and costs (feeds KPI I1.4 - Current Workarounds)

**Probing Questions:**
- What triggered the need to scale?
- What options did you consider?
- What did you ultimately do? (hire, tools, process changes)
- What was the timeline and cost?
- How well did it work?
- What would you do differently?

**Red Flags to Listen For:**
- Hasn't faced scaling challenge (low growth or low demo volume)
- Scaling was easy and painless
- Has unlimited budget/resources

**What Good Looks Like:**
- Recent scaling challenge
- Felt significant pressure/urgency
- Expensive or slow solution (hired AEs)
- Still looking for better approach

---

#### Q7: When did you last have a conversation with your team about improving the demo process? What specifically were you discussing?

**Purpose:** Validates problem recency and active seeking behavior (feeds KPI I1.2 - Problem Frequency)

**Probing Questions:**
- Who was in the conversation?
- What triggered the conversation?
- What ideas came up?
- What did you decide to do?
- Have you had follow-up conversations?

**Red Flags to Listen For:**
- Can't remember recent conversation
- "We discussed it a long time ago"
- No active initiative to solve

**What Good Looks Like:**
- Conversation within last month
- Multiple stakeholders involved
- Active project or initiative underway
- Budget allocated or being requested

---

#### Q8: If you could wave a magic wand and fix one thing about your demo process, what would it be? Why that specific thing?

**Purpose:** Reveals top-of-mind pain and desired outcomes (feeds KPI I1.1 - Articulation Clarity)

**Probing Questions:**
- What would change about your results if you fixed that?
- What have you tried to fix it?
- How much would that improvement be worth to you?
- What's preventing you from fixing it now?

**Red Flags to Listen For:**
- Vague wish ("make it better")
- Feature request that doesn't align with your solution
- Problem unrelated to scheduling/delivery

**What Good Looks Like:**
- Specific, concrete improvement
- Directly related to core problem
- Can articulate business impact
- Aligns with your solution capabilities

---

### SECTION 2: Solution Exploration (Q9-Q16, 20 minutes)

**Focus:** Present solution concept neutrally. Assess solution resonance and competitive displacement. Check outcome alignment.

**Transition Statement:**
"Thanks for sharing all that context. I want to shift gears and get your reaction to a potential solution approach. I'm going to describe a concept—this is not a pitch, and I genuinely want your honest reaction, including if you think it's a bad idea."

---

#### Q9: [Present Solution Concept - 60 seconds]

**Solution Pitch Script:**
"Imagine if when a prospect requested a demo, an AI agent could join a video call with them within 5 minutes—24/7, no scheduling needed. The AI has a voice, can have a two-way conversation, and actually controls your real product in real-time to show them exactly what they ask about. It's connected to your CRM, so it knows who they are, what they've looked at, and can qualify them and log everything automatically.

The prospect gets an instant, personalized demo on their schedule. Your team gets qualified leads without spending AE time on early-stage demos."

**Pause for reaction. Don't pitch further. Listen.**

**Purpose:** Introduces solution context for questions 10-16

---

#### Q10: What's your immediate gut reaction to that concept?

**Purpose:** Gauges authentic emotional response (feeds KPI I1.6 - Solution Resonance)

**Probing Questions:**
- What do you like about it?
- What concerns you?
- What's the first question that comes to mind?
- Would your prospects be open to talking to an AI?

**Red Flags to Listen For:**
- Polite but unenthusiastic ("That's interesting...")
- Immediate objections that kill the concept
- "Prospects would never accept AI"

**What Good Looks Like:**
- Genuine excitement or strong curiosity
- "That would solve [specific problem we discussed]"
- Asks detailed follow-up questions
- Starts imagining use cases

---

#### Q11: Tell me about a recent demo that this AI approach would have handled well. What about one that it wouldn't have handled well?

**Purpose:** Tests solution fit against real scenarios (feeds KPI I1.6 - Solution Resonance)

**Probing Questions:**
- What made the first one a good fit?
- What made the second one a bad fit?
- What percentage of your demos fit the "good fit" category?
- What would the AI need to handle more of your demos?

**Red Flags to Listen For:**
- Most demos wouldn't fit (complex enterprise, highly technical)
- Can't think of good fit scenarios
- Unique/custom demos every time

**What Good Looks Like:**
- Clear use cases for 50%+ of demos
- Identifies specific demo types (early-stage, standard flow)
- Sees opportunity to free up AE time for complex deals

---

#### Q12: If you could use something like this, what would need to be true about the AI's capabilities for you to trust it with your prospects?

**Purpose:** Uncovers must-have requirements and deal-breakers (feeds KPI I1.6 - Solution Resonance)

**Probing Questions:**
- How would you evaluate if it's ready?
- What mistakes would be unacceptable?
- How much control would you need over what it says/shows?
- What training/setup would you expect to do?

**Red Flags to Listen For:**
- Unrealistic requirements (100% human-like, zero errors)
- "I'd need to be on every call" (defeats the purpose)
- Long list of technical requirements that are very difficult to build

**What Good Looks Like:**
- Reasonable bar for success (better than average SDR)
- Willing to accept some limitations
- Focus on business outcomes over perfection

---

#### Q13: If you started using this tomorrow, what current tools or processes would you replace or eliminate?

**Purpose:** Assesses competitive displacement and ROI calculation (feeds KPI I1.7 - Competitive Displacement)

**Probing Questions:**
- What are you currently paying for those tools?
- Would you still need [specific tool]?
- Could you reduce headcount or redeploy people?
- What else would change in your workflow?

**Red Flags to Listen For:**
- "It would be additive" (no displacement = hard to justify ROI)
- Can't identify what they'd cut
- Currently not spending anything on this problem

**What Good Looks Like:**
- Identifies specific tools/costs to eliminate
- Talks about redeploying AE time to high-value deals
- Calculates potential savings unprompted

---

#### Q14: What concerns would your team have about using AI for demos? What about your prospects?

**Purpose:** Uncovers objections and adoption barriers (feeds KPI I1.6 - Solution Resonance)

**Probing Questions:**
- Which concerns are deal-breakers vs manageable?
- How would you introduce it to prospects?
- What would make your AEs comfortable with it?
- How would you measure if it's working?

**Red Flags to Listen For:**
- "Our AEs would never accept this" (adoption risk)
- "Prospects want human interaction, period"
- Strong concern about brand risk

**What Good Looks Like:**
- Acknowledges concerns but sees mitigation strategies
- More excited about benefits than worried about risks
- Already thinking about change management

---

#### Q15: Think about your ideal outcome from improving your demo process. Does this approach get you to that outcome? What's missing?

**Purpose:** Tests outcome alignment with solution (feeds KPI I1.6 - Solution Resonance)

**Probing Questions:**
- What metrics would you track to know it's working?
- What would success look like in 6 months?
- What would prevent you from achieving that outcome with this?
- What additional features would make it a complete solution?

**Red Flags to Listen For:**
- Desired outcome doesn't match solution capabilities
- Long list of missing critical features
- "It's a nice-to-have, not transformational"

**What Good Looks Like:**
- Solution maps directly to desired outcome
- Realistic expectations about what it should deliver
- Excited about specific metrics (response time, conversion rate)

---

#### Q16: If you were evaluating a solution like this, who else would need to be involved in the decision? What would each person care about most?

**Purpose:** Maps buying process and decision criteria (feeds KPI I1.7 - Competitive Displacement)

**Probing Questions:**
- Who has final budget authority?
- Who would be the champion vs skeptic?
- What would each stakeholder need to see to approve?
- How long does a purchase like this typically take?
- What approval process would it go through?

**Red Flags to Listen For:**
- Complex approval process (6+ months, many stakeholders)
- Interviewee has no influence over purchase
- Budget owner is several levels removed

**What Good Looks Like:**
- Clear decision-making process
- Interviewee is champion or budget owner
- Can articulate each stakeholder's concerns
- Reasonable timeline (1-3 months)

---

### SECTION 3: Willingness to Pay (Q17-Q27, 20 minutes)

**Focus:** Explore current spend and budget. Van Westendorp pricing questions. Purchase intent assessment.

**Transition Statement:**
"I want to shift to understanding the economics and budget considerations. These questions help me understand what a solution like this should cost to be viable for companies like yours."

---

#### Q17: Walk me through your current budget allocation for sales tools and demo-related costs. What are you spending today?

**Purpose:** Establishes baseline spend and budget context (feeds KPI I1.8 - Budget Available)

**Probing Questions:**
- CRM costs?
- Sales engagement platform (Outreach, Salesloft)?
- Demo automation tools (Consensus, Demostack)?
- Scheduling tools (Calendly, Chili Piper)?
- What's your total sales software budget?
- How much of that is flexible/discretionary?

**Red Flags to Listen For:**
- Very low spend (suggests budget constraints)
- "I don't know our budget" (not a budget owner)
- Budget is fully allocated with no flex

**What Good Looks Like:**
- Knows specific costs
- Identifies line items that could be reallocated
- Has discretionary budget for new tools

---

#### Q18: If you could solve the demo scheduling and delivery problem, what would that be worth to your business in dollars? How do you calculate that?

**Purpose:** Uncovers value perception and ROI framework (feeds KPI I1.10 - Purchase Intent)

**Probing Questions:**
- What revenue are you losing to delayed demos?
- What does AE time cost when spent on unqualified demos?
- What's the value of faster pipeline velocity?
- How much would you pay to reduce no-shows by 50%?

**Red Flags to Listen For:**
- Can't articulate value in dollar terms
- Value is very low (<$10K/year)
- Vague or hand-wavy calculations

**What Good Looks Like:**
- Concrete math with specific assumptions
- Values their time and team's time realistically
- Calculates based on deals won, not just costs saved

---

#### Q19: Tell me about the last sales tool you purchased. What did it cost, and how did you justify the investment?

**Purpose:** Establishes purchasing patterns and price anchoring (feeds KPI I1.8 - Budget Available)

**Probing Questions:**
- What tool was it?
- What pricing model? (per-seat, flat fee, usage-based)
- What was your evaluation process?
- How did you build the business case?
- Was the price a barrier? Why or why not?

**Red Flags to Listen For:**
- Last purchase was years ago
- Very low price point (<$1K/year)
- Painful approval process that took 12+ months

**What Good Looks Like:**
- Recent purchase (within 12 months)
- Mid-market price range ($10K-$100K/year)
- Clear ROI justification
- Efficient purchase process

---

#### Q20: What pricing model makes the most sense to you for a solution like this? Why?

**Purpose:** Uncovers pricing model preferences (feeds KPI I1.9 - Willingness to Pay)

**Probing Questions:**
- Per-seat vs flat fee vs usage-based?
- Monthly vs annual contract?
- Why does that model align with how you think about value?
- What pricing model would be a non-starter for you?

**Red Flags to Listen For:**
- Only willing to pay per-usage (signals uncertainty)
- "We never do annual contracts"
- Prefers very low commitment (month-to-month)

**What Good Looks Like:**
- Comfortable with annual commitments
- Sees value in flat fee or per-seat (predictability)
- Willing to pay for value, not just cost-plus

---

### Van Westendorp Price Sensitivity Meter (Q21-Q24)

**Setup Statement:**
"I'm going to ask you four pricing questions. There are no right answers—I'm trying to understand the range of prices that would make sense for a solution like this. Assume it delivers everything we discussed: instant demos 24/7, AI-controlled product navigation, CRM integration, and qualification."

---

#### Q21: At what price would you consider this solution to be so expensive that you would not consider buying it? (Too Expensive)

**Purpose:** Identifies price ceiling (feeds KPI I1.9 - Willingness to Pay)

**Probing Questions:**
- What makes that the cutoff?
- Is there any scenario where you'd pay more?
- What would need to be true to justify more?

**Red Flags to Listen For:**
- Very low threshold (<$5K/year for mid-market)
- "I'd never pay for this"

**What Good Looks Like:**
- Reasonable ceiling based on company size
- Clear logic for the threshold

---

#### Q22: At what price would you consider this solution to be priced so low that you would feel the quality couldn't be very good? (Too Cheap)

**Purpose:** Identifies credibility floor (feeds KPI I1.9 - Willingness to Pay)

**Probing Questions:**
- Why would low price signal low quality?
- What's the minimum you'd expect to pay for this capability?

**Red Flags to Listen For:**
- Very high floor (suggests skepticism)
- "Free would be too cheap" (unrealistic)

**What Good Looks Like:**
- Reasonable floor ($2K-$10K/year)
- Understands this is sophisticated technology

---

#### Q23: At what price would you consider this solution starting to get expensive, but you'd still consider buying it? (Getting Expensive)

**Purpose:** Identifies upper bound of optimal range (feeds KPI I1.9 - Willingness to Pay)

**Probing Questions:**
- At this price, what would your evaluation process look like?
- Would you need additional approval?
- What ROI would you need to justify it?

**Red Flags to Listen For:**
- Higher than "too expensive" (illogical)
- Very low threshold

**What Good Looks Like:**
- Clear price point with reasoning
- Explains how evaluation changes at this price

---

#### Q24: At what price would you consider this solution to be a bargain—a great buy for the money? (Bargain)

**Purpose:** Identifies lower bound of optimal range (feeds KPI I1.9 - Willingness to Pay)

**Probing Questions:**
- Why is that a great deal?
- What value are you comparing it to?
- At this price, how quickly would you buy?

**Red Flags to Listen For:**
- Lower than "too cheap" (illogical)
- Unrealistically low expectations

**What Good Looks Like:**
- Reasonable price based on value delivered
- Shows excitement at this price point

---

#### Q25: Based on everything we've discussed, how likely would you be to purchase a solution like this in the next 6 months? Rate from 1-5, where 1 is "definitely not" and 5 is "definitely yes."

**Purpose:** Quantifies purchase intent (feeds KPI I1.10 - Purchase Intent Score)

**Probing Questions:**
- What would move you from [X] to [X+1]?
- What would need to happen in the next 6 months for you to buy?
- What would prevent you from buying?
- Who would need to approve it?

**Red Flags to Listen For:**
- Score of 1-2 (no intent)
- "I'd need to see it working first" (long sales cycle)
- Can't articulate path to purchase

**What Good Looks Like:**
- Score of 4-5 with clear reasoning
- Describes specific buying trigger
- Has budget and authority

---

#### Q26: If this solution launched in the next 30 days, would you want to be part of an early beta or pilot program? What would you need to see in a pilot to move forward?

**Purpose:** Identifies early adopters and pilot readiness (feeds KPI I1.10 - Purchase Intent)

**Probing Questions:**
- What would a successful pilot look like?
- How long would you need to evaluate?
- What metrics would you track?
- What would need to happen to convert to paid?

**Red Flags to Listen For:**
- "Not interested in being early"
- 6+ month pilot required
- Wants free forever

**What Good Looks Like:**
- Excited about early access
- Reasonable pilot scope (30-60 days)
- Clear success criteria
- Willing to pay for pilot or commit to convert

---

#### Q27: Is there anything I didn't ask that I should have? Any other thoughts on this problem or solution?

**Purpose:** Captures unexpected insights and builds relationship (general insight)

**Probing Questions:**
- What did I miss?
- What advice would you give me?
- Who else should I talk to?

**Red Flags to Listen For:**
- Reveals major concern they were holding back
- Mentions competitor they love that you didn't know about

**What Good Looks Like:**
- Offers intros to other relevant people
- Shares additional pain points
- Offers to stay involved (advisor, pilot, champion)

---

### Closing (5 minutes)

**Closing Script:**
"This has been incredibly valuable—thank you so much for your time and candor. A few final things:

1. **Follow-up:** I'll send you a summary of what I'm learning from these conversations in the next few weeks. Is that helpful?

2. **Next steps:** [If high purchase intent] Would you be open to another conversation once we have a prototype ready to show you?

3. **Referrals:** Who else in your network should I talk to who faces similar challenges?

4. **Thank you:** I'll send the [$50 gift card / early access details / benchmark report] via email today.

Thanks again—this was really helpful."

**Post-Interview (Immediately):**
- [ ] Fill out tracking spreadsheet while fresh
- [ ] Note 2-3 key quotes
- [ ] Flag as potential early adopter (Y/N)
- [ ] Send thank you email within 24 hours
- [ ] Send referral requests if appropriate

---

## Tracking Template

See separate `interview-tracking-template.md` file for the spreadsheet structure.

**Key fields to capture:**
- Interview metadata (date, persona, company)
- Problem validation scores (pain 0-10, frequency, workarounds)
- Solution fit scores (resonance, displacement, outcome alignment)
- Pricing data (Van Westendorp 4 prices, current spend)
- Purchase intent (1-5 score, timeline, early adopter Y/N)
- Qualitative notes and quotes

---

## The Mom Test Reminders

### Core Principles

**1. Talk about their life, not your idea**
- ❌ Bad: "Would you use a product that delivers instant AI demos?"
- ✅ Good: "Tell me about the last time you had a demo no-show."

**2. Ask about specifics in the past**
- ❌ Bad: "How often do you have demo scheduling issues?"
- ✅ Good: "When was the last time a lead went cold due to demo delays?"

**3. Listen more than you talk**
- Target: 70% listening, 30% talking
- Use probes to go deeper, not to redirect
- Silence is okay—let them think and elaborate

**4. Don't pitch**
- Present solution concept neutrally
- Listen for genuine resonance, not politeness
- It's okay if they don't like it—that's valuable data

**5. Dig into emotions**
- When they mention frustration, probe: "Tell me more about that"
- When they get excited, probe: "What makes that important to you?"
- Pain is more important than features

**6. Ask for commitments, not compliments**
- ❌ Bad: "Would this be valuable to you?"
- ✅ Good: "Would you participate in a paid pilot in the next 30 days?"

---

## Interview Best Practices

### During the Interview

**DO:**
- Record (with permission) for later review
- Take brief notes on key quotes and reactions
- Follow tangents if they reveal deeper pain
- Ask "Why?" and "Tell me more" frequently
- Probe on specific dollar amounts and timelines
- Listen for emotion (frustration, excitement, anxiety)

**DON'T:**
- Pitch or try to convince them
- Lead the witness ("Don't you think that...")
- Fill awkward silences too quickly
- Defend your solution against criticism
- Skip questions that feel uncomfortable
- Interrupt when they're sharing valuable context

### Reading Between the Lines

**High-Intent Signals:**
- Asks detailed implementation questions
- Introduces you to colleagues
- Describes specific budget and timeline
- Shares proprietary data/metrics
- Offers to stay involved

**Low-Intent Signals:**
- Vague, polite responses
- Can't articulate specific pain points
- No urgency or timeline
- "I'd need to see it working first"
- Won't commit to pilot/beta

**Red Flags:**
- Problem is hypothetical, not experienced
- Happy with current workarounds
- No budget authority or influence
- Complex buying process (6+ month)
- Solution doesn't fit their workflow

---

## Success Metrics for Interview Program

**Quantitative Goals:**
- 10-15 completed interviews
- 70%+ report pain severity ≥7/10
- 60%+ show strong purchase intent (≥4/5)
- 3-5 early adopter candidates identified
- Clear optimal price range from Van Westendorp

**Qualitative Goals:**
- Validated core problem assumptions
- Identified 2-3 must-have solution features
- Uncovered competitive alternatives
- Mapped buying process and decision criteria
- Built relationships with potential design partners

---

**Interview Guide Version:** 1.0
**Last Updated:** 2025-12-08
**Compatible with:** Recipe 1.1 (Problem Validation)

---

## Quick Reference: Question Mapping to KPIs

| KPI | Questions | What to Measure |
|-----|-----------|----------------|
| I1.1 - Articulation Clarity | Q1, Q8 | Can they clearly describe the problem? |
| I1.2 - Problem Frequency | Q2, Q7 | How often does this happen? |
| I1.3 - Problem Severity | Q3, Q4 | How painful is it? (0-10 scale, impact on revenue) |
| I1.4 - Current Workarounds | Q5, Q6 | What are they using today? What's missing? |
| I1.5 - Competitive Landscape | Q5, Q13 | What alternatives exist? |
| I1.6 - Solution Resonance | Q10-Q15 | Do they connect with the solution? |
| I1.7 - Competitive Displacement | Q13, Q16 | Would they replace current solutions? |
| I1.8 - Budget Available | Q17, Q19 | Do they have budget? How much? |
| I1.9 - Willingness to Pay | Q20-Q24 | What would they pay? (Van Westendorp) |
| I1.10 - Purchase Intent | Q25-Q26 | Will they actually buy? When? |

---

Good luck with your interviews! Remember: you're there to learn, not to sell. The best interviews are conversations where the prospect talks 70% of the time and you uncover truths you didn't expect.
